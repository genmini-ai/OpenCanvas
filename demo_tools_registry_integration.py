#!/usr/bin/env python3
"""
Demo: Tools Registry Integration with Evolution System
Shows how the evolution system discovers, tracks, and manages tools
"""

import sys
import json
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from opencanvas.evolution.tools_registry_manager import ToolsRegistryManager, ToolDiscoveryTracker
from opencanvas.evolution.multi_agent.implementation_agent import ImplementationAgent

def demo_tools_registry_integration():
    """Demonstrate the complete tools registry workflow"""
    
    print("ğŸ”§ TOOLS REGISTRY INTEGRATION DEMO")
    print("=" * 50)
    
    # Step 1: Initialize registry manager
    print("\nğŸ“‹ Step 1: Initialize Tools Registry")
    registry_manager = ToolsRegistryManager("demo_tools_registry.md")
    discovery_tracker = ToolDiscoveryTracker()
    discovery_tracker.set_iteration(1)
    
    print(f"âœ… Registry initialized at: demo_tools_registry.md")
    
    # Step 2: Simulate tool discovery during evolution
    print("\nğŸ” Step 2: Simulate Tool Discovery")
    
    # Sample tool specifications that might be discovered
    discovered_tools = [
        {
            "name": "Citation Verification Tool",
            "purpose": "Detect and prevent fake citations in presentations",
            "target_problem": "Analysis shows 20% of presentations contain fake author names",
            "implementation": {
                "class_name": "CitationVerificationTool",
                "main_method": "verify_citation",
                "params": "author, title, venue, year",
                "description": "Validate citations against academic databases",
                "logic": "return confidence_score, validation_issues"
            },
            "expected_impact": "high",
            "complexity": "low",
            "integration_points": ["post_generation_validation"],
            "validation_method": "A/B test with/without citation validation",
            "cost_estimate": "~$0.01 per generation (database lookup)",
            "speed_impact": "minimal (<500ms additional)"
        },
        {
            "name": "Chart Readability Validator",
            "purpose": "Ensure charts and visualizations are readable",
            "target_problem": "Visual evaluation shows poor chart readability scores",
            "implementation": {
                "class_name": "ChartValidationTool", 
                "main_method": "validate_chart",
                "params": "chart_html, context",
                "description": "Render and analyze chart readability",
                "logic": "return readability_score, improvement_suggestions"
            },
            "expected_impact": "high",
            "complexity": "medium",
            "integration_points": ["during_generation", "post_generation"],
            "validation_method": "Chart readability score comparison",
            "cost_estimate": "~$0.02 per chart (rendering + analysis)",
            "speed_impact": "moderate (~2s per chart)"
        },
        {
            "name": "Content Balance Analyzer",
            "purpose": "Detect and fix text walls and sparse slides",
            "target_problem": "Slides often have poor text-visual balance",
            "implementation": {
                "class_name": "ContentBalanceAnalyzer",
                "main_method": "analyze_balance",
                "params": "slide_html",
                "description": "Analyze word count, visual ratio, readability",
                "logic": "return balance_score, redistribution_suggestions"
            },
            "expected_impact": "medium",
            "complexity": "low",
            "integration_points": ["post_generation_analysis"],
            "validation_method": "Content balance score tracking",
            "cost_estimate": "negligible (text analysis only)",
            "speed_impact": "minimal (<100ms per slide)"
        }
    ]
    
    # Record tool discoveries
    for tool in discovered_tools:
        discovery_tracker.record_tool_discovery(tool)
        print(f"  ğŸ”§ Discovered: {tool['name']}")
    
    print(f"âœ… Discovered {len(discovered_tools)} tools in iteration 1")
    
    # Step 3: Prioritize discoveries
    print("\nğŸ“Š Step 3: Prioritize Tool Discoveries")
    prioritized_tools = discovery_tracker.prioritize_discoveries()
    
    print("Tool Priority Ranking:")
    for i, tool in enumerate(prioritized_tools, 1):
        impact = tool['expected_impact']
        complexity = tool['complexity']
        print(f"  {i}. {tool['name']} (Impact: {impact}, Complexity: {complexity})")
    
    # Step 4: Add tools to registry
    print("\nğŸ“ Step 4: Add Tools to Registry")
    for tool in prioritized_tools:
        registry_manager.add_proposed_tool(tool)
        print(f"  âœ… Added to registry: {tool['name']}")
    
    # Step 5: Simulate testing and results
    print("\nğŸ§ª Step 5: Simulate Tool Testing")
    
    # Simulate successful tool adoption
    citation_tool_results = {
        "purpose": "Detect and prevent fake citations",
        "test_period": "2024-07-30 to 2024-08-05",
        "metrics": {
            "fake_citations_detected": "95% accuracy",
            "false_positives": "< 3%",
            "generation_time_impact": "+0.2 seconds",
            "quality_score_improvement": "+0.4 in accuracy dimension"
        },
        "adopted": True,
        "version": "1.2",
        "iteration": 1,
        "implementation_details": "CitationVerificationTool integrated into post-generation pipeline",
        "impact_summary": "Reduces fake citations from 20% to <1%",
        "metrics_summary": "95% detection accuracy, minimal speed impact"
    }
    
    registry_manager.update_tool_test_results("Citation Verification Tool", citation_tool_results)
    print("  âœ… Citation Verification Tool: ADOPTED")
    
    # Simulate rejected tool
    chart_tool_results = {
        "purpose": "Validate chart readability",
        "test_period": "2024-08-01 to 2024-08-07", 
        "metrics": {
            "chart_quality_improvement": "+0.2 visual score",
            "generation_time_impact": "+5 seconds per presentation",
            "cost_increase": "+40% API costs",
            "reliability": "78% success rate"
        },
        "adopted": False,
        "rejection_reason": "speed and cost impact too high for MVP",
        "lessons_learned": "Chart validation requires lighter-weight approach or async processing"
    }
    
    registry_manager.update_tool_test_results("Chart Readability Validator", chart_tool_results)
    print("  âŒ Chart Readability Validator: REJECTED (speed/cost concerns)")
    
    # Step 6: Generate summary
    print("\nğŸ“ˆ Step 6: Generate Tools Summary")
    summary = registry_manager.generate_tool_summary()
    
    print(f"Registry Summary:")
    print(f"  ğŸ“‹ Current Tools: {summary['total_current']}")
    print(f"  ğŸ§ª Rejected Tools: {summary['total_rejected']}")
    print(f"  ğŸ“ Registry Location: {summary['registry_path']}")
    
    # Step 7: Show registry contents
    print("\nğŸ“– Step 7: Registry File Created")
    with open("demo_tools_registry.md", "r") as f:
        lines = f.readlines()
    
    print("Registry file preview (first 20 lines):")
    for i, line in enumerate(lines[:20], 1):
        print(f"  {i:2d}: {line.rstrip()}")
    
    if len(lines) > 20:
        print(f"  ... (and {len(lines) - 20} more lines)")
    
    print(f"\nğŸ‰ Tools Registry Demo Complete!")
    print(f"ğŸ“ Full registry saved to: demo_tools_registry.md")
    print(f"ğŸ”§ Evolution system can now systematically discover and track tools!")
    
    return {
        "tools_discovered": len(discovered_tools),
        "tools_adopted": 1,
        "tools_rejected": 1,
        "registry_file": "demo_tools_registry.md"
    }

def show_integration_architecture():
    """Show how the registry integrates with the evolution system"""
    
    print("\nğŸ—ï¸ INTEGRATION ARCHITECTURE")
    print("=" * 50)
    
    architecture = """
Evolution System Tool Integration Flow:

1. Multi-Agent Analysis
   â””â”€â”€ Identifies quality gaps and patterns
   
2. Implementation Agent
   â”œâ”€â”€ Designs prompt improvements
   â””â”€â”€ ğŸ†• Proposes new tools for gaps that prompts can't solve
   
3. Tools Registry Manager
   â”œâ”€â”€ Tracks proposed tools
   â”œâ”€â”€ Records test results  
   â””â”€â”€ Manages tool lifecycle
   
4. Tool Development Pipeline
   â”œâ”€â”€ A/B Testing Framework
   â”œâ”€â”€ Performance Validation
   â””â”€â”€ Integration Decisions
   
5. Production Integration
   â”œâ”€â”€ Adopted tools â†’ Current Tools section
   â””â”€â”€ Rejected tools â†’ Lessons learned

Tool Categories Discovered:
â”œâ”€â”€ ğŸ” Validation Tools (Citation verification, Chart validation)
â”œâ”€â”€ ğŸ“Š Analysis Tools (Content balance, Flow analysis)  
â”œâ”€â”€ ğŸ¨ Enhancement Tools (Visual consistency, Design optimization)
â””â”€â”€ ğŸ§  Intelligence Tools (Domain knowledge, Fact checking)
"""
    
    print(architecture)

if __name__ == "__main__":
    try:
        results = demo_tools_registry_integration()
        show_integration_architecture()
        
        print(f"\nâœ… Demo completed successfully!")
        print(f"   Discovered: {results['tools_discovered']} tools")
        print(f"   Adopted: {results['tools_adopted']} tools") 
        print(f"   Rejected: {results['tools_rejected']} tools")
        print(f"   Registry: {results['registry_file']}")
        
    except Exception as e:
        print(f"\nâŒ Demo failed: {e}")
        import traceback
        traceback.print_exc()