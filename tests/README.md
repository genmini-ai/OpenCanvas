# OpenCanvas Test Suite

This directory contains comprehensive tests for the OpenCanvas presentation generation system.

## Quick Start

```bash
# From root directory (recommended)
python run_tests.py light              # Quick validation
python run_tests.py topic              # Topic tests only
python run_tests.py pdf                # PDF tests only

# From tests directory
cd tests
python run_e2e_tests.py light          # Quick validation
```

## Test Structure

### üìÅ Test Files

- **`test_e2e_pipeline.py`** - Complete end-to-end pipeline tests
- **`test_topics.py`** - Topic-based generation tests
- **`test_pdfs.py`** - PDF-based generation tests  
- **`test_conversion.py`** - HTML to PDF conversion tests

### üéØ E2E Test Suite

The main test suite (`test_e2e_pipeline.py`) includes:

#### **Topic-Based Tests (5 test cases)**
1. **Sustainable Energy Solutions** - Corporate presentation with natural earth theme
2. **AI in Healthcare** - Academic presentation with professional blue theme
3. **Quantum Computing Fundamentals** - Educational presentation with modern contemporary theme
4. **Climate Change Mitigation** - Conference talk with clean minimalist theme
5. **Blockchain Technology** - Pitch deck with bold high contrast theme

#### **PDF-Based Tests (5 arXiv papers)**
1. **Scaling Laws for Neural Language Models** - `arxiv:2505.20286`
2. **Chain-of-Thought Reasoning** - `arxiv:2410.17891`
3. **AI Safety and Alignment** - `arxiv:2502.02533`
4. **Transformer Architecture Analysis** - `arxiv:2310.13855`
5. **Large Language Model Training** - `arxiv:2307.08123`

## Running Tests

### Prerequisites

1. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up environment variables**:
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

3. **Required API keys**:
   - `ANTHROPIC_API_KEY` - Required for generation
   - `GEMINI_API_KEY` - Default for evaluation (recommended)
   - `OPENAI_API_KEY` - Optional for GPT evaluation
   - `BRAVE_API_KEY` - Optional for enhanced research

### Quick Tests

#### Test Gemini Integration
```bash
python test_gemini_integration.py
```

#### Test Individual Components
```bash
# Topic generation tests
python -m pytest tests/test_topics.py -v

# PDF generation tests  
python -m pytest tests/test_pdfs.py -v

# Conversion tests
python -m pytest tests/test_conversion.py -v
```

### Full E2E Test Suite

#### Run Complete Suite
```bash
python run_e2e_tests.py
```

#### Run Specific Test Types
```bash
# Topic-based tests only
python run_e2e_tests.py topic

# PDF-based tests only
python run_e2e_tests.py pdf

# Full suite (default)
python run_e2e_tests.py full
```

#### Light Testing Mode (Fast)
For quick validation, use light mode which runs only 1 test per type:

```bash
# Light mode - full suite (2 tests total, ~5 minutes)
python run_e2e_tests.py light

# Light mode - topic tests only (1 test, ~2 minutes)
python run_e2e_tests.py topic light

# Light mode - PDF tests only (1 test, ~3 minutes)
python run_e2e_tests.py pdf light

# You can also combine arguments in any order
python run_e2e_tests.py light topic
```

#### Resume/Caching (Skip Existing)
The test suite automatically detects existing slides and skips generation for faster iteration:

```bash
# Normal run - will skip generation if slides exist
python run_e2e_tests.py light

# Force regeneration - ignores existing slides
python run_e2e_tests.py light force

# Force regeneration for specific test type
python run_e2e_tests.py topic force
python run_e2e_tests.py pdf force

# Combine all options
python run_e2e_tests.py topic light force
```

**How Caching Works:**
- Tests check for existing `presentation.html` and `presentation.pdf` in organized output directories
- If both files exist, generation and conversion stages are skipped
- Evaluation still runs (unless results already exist)
- Use `force` flag to regenerate everything from scratch

#### Using pytest
```bash
# Run all E2E tests
python -m pytest tests/test_e2e_pipeline.py -v

# Run specific test types
python -m pytest tests/test_e2e_pipeline.py::TestE2EPipeline::test_topic_based_e2e_individual -v
python -m pytest tests/test_e2e_pipeline.py::TestE2EPipeline::test_pdf_based_e2e_individual -v
```

## Test Pipeline Stages

Each E2E test runs through these stages:

### Topic-Based Pipeline
1. **üìù Generation** - Generate presentation from topic using TopicGenerator
2. **üìÑ Conversion** - Convert HTML to PDF using PresentationConverter
3. **üìÅ Organization** - Organize files using organized output structure
4. **üîç Evaluation** - Evaluate presentation using PresentationEvaluator

### PDF-Based Pipeline
1. **üìë Generation** - Generate presentation from PDF using PDFGenerator
2. **üìÑ Conversion** - Convert HTML to PDF using PresentationConverter
3. **üì• Source Download** - Download original arXiv PDF for evaluation
4. **üìÅ Organization** - Organize files using organized output structure
5. **üîç Evaluation** - Evaluate presentation with source PDF comparison

## Test Results

### Output Structure
Tests create organized output directories:
```
test_output/
‚îú‚îÄ‚îÄ test_1_sustainable_energy_solutions/
‚îÇ   ‚îú‚îÄ‚îÄ slides/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ presentation.html
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ presentation.pdf
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation_results.json
‚îÇ   ‚îî‚îÄ‚îÄ sources/
‚îÇ       ‚îú‚îÄ‚îÄ source_content.txt
‚îÇ       ‚îî‚îÄ‚îÄ source.pdf
‚îî‚îÄ‚îÄ test_2_ai_in_healthcare/
    ‚îî‚îÄ‚îÄ ...
```

### Results File
Detailed results are saved to `e2e_test_results.json`:
```json
{
  "full_suite": true,
  "topic_based": {
    "total_tests": 5,
    "passed": 5,
    "failed": 0,
    "results": { ... }
  },
  "pdf_based": {
    "total_tests": 5,
    "passed": 5,
    "failed": 0,
    "results": { ... }
  },
  "summary": {
    "total_tests": 10,
    "total_passed": 10,
    "total_failed": 0
  }
}
```

## Configuration

### Evaluation Providers

The test suite uses the default evaluation provider from your `.env` file:

```bash
# Gemini (default - fast and cost-effective)
EVALUATION_PROVIDER=gemini
EVALUATION_MODEL=gemini-2.5-flash
GEMINI_API_KEY=your_gemini_key

# Claude (high quality)
EVALUATION_PROVIDER=claude
EVALUATION_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_API_KEY=your_anthropic_key

# GPT (alternative)
EVALUATION_PROVIDER=gpt
EVALUATION_MODEL=gpt-4o-mini
OPENAI_API_KEY=your_openai_key
```

### Test Customization

You can modify test cases in `test_e2e_pipeline.py`:

- **`TOPIC_TEST_CASES`** - Add/modify topic-based test scenarios
- **`ARXIV_TEST_CASES`** - Add/modify PDF-based test scenarios

## Troubleshooting

### Common Issues

1. **API Key Missing**
   ```
   ‚ùå ANTHROPIC_API_KEY not provided
   ```
   **Solution**: Add API key to `.env` file

2. **Evaluation Skipped**
   ```
   stages.evaluation.status: "skipped"
   ```
   **Solution**: Add evaluation provider API key

3. **PDF Download Failed**
   ```
   stages.source_download.status: "failed"
   ```
   **Solution**: Check internet connection and arXiv availability

4. **Conversion Failed**
   ```
   stages.conversion.status: "failed"
   ```
   **Solution**: Install Playwright browsers: `playwright install chromium`

### Debug Mode

For detailed debugging, set log level:
```bash
export LOG_LEVEL=DEBUG
python run_e2e_tests.py
```

## Performance

### Expected Runtime

#### Full Mode (All Tests)
- **Topic-based test**: ~2-3 minutes per test
- **PDF-based test**: ~3-4 minutes per test (includes PDF download)
- **Full suite**: ~25-35 minutes total

#### Light Mode (1 Test Per Type)
- **Topic-based test**: ~2-3 minutes (1 test)
- **PDF-based test**: ~3-4 minutes (1 test)
- **Light suite**: ~5-7 minutes total

#### Cached Mode (Existing Slides)
- **Topic-based test**: ~30 seconds (evaluation only)
- **PDF-based test**: ~45 seconds (evaluation only)
- **Cached light suite**: ~1-2 minutes total

### Resource Usage
- **Storage**: ~50-100MB per test (temporary files)
- **Network**: ~10-20MB per PDF download
- **API calls**: ~5-10 calls per test (generation + evaluation)
- **Cached runs**: ~1-2 API calls per test (evaluation only)

## Contributing

### Adding New Tests

1. **Topic-based tests**: Add to `TOPIC_TEST_CASES` in `test_e2e_pipeline.py`
2. **PDF-based tests**: Add to `ARXIV_TEST_CASES` in `test_e2e_pipeline.py`
3. **Component tests**: Add to respective `test_*.py` files

### Test Guidelines

- Use descriptive test names
- Include variety in themes and purposes
- Test edge cases and error conditions
- Verify file existence and content quality
- Check evaluation scores are reasonable

## CI/CD Integration

The test suite can be integrated into CI/CD pipelines:

```yaml
# GitHub Actions example
- name: Run E2E Tests
  run: |
    python run_e2e_tests.py
  env:
    ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
    GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
``` 